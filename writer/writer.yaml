apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: ScheduledSparkApplication
metadata:
  name: spark-minio-k8s-writer
  namespace: spark-operator
spec:
  schedule: "@every 1m"
  concurrencyPolicy: Forbid
  template:
    type: Scala
    mode: cluster
    image: "wcygan/spark-minio-k8s-writer"
    imagePullPolicy: Always
    mainClass: org.example.App
    mainApplicationFile: "local:///opt/spark/examples/jars/writer.jar"
    sparkVersion: "3.5.0"
    restartPolicy:
      type: Never
    volumes:
      - name: "test-volume"
        hostPath:
          path: "/tmp"
          type: Directory
    driver:
      cores: 1
      coreLimit: "1200m"
      memory: "512m"
      labels:
        version: 3.5.0
      serviceAccount: spark-runner
      volumeMounts:
        - name: "test-volume"
          mountPath: "/tmp"
    executor:
      cores: 1
      instances: 1
      memory: "512m"
      labels:
        version: 3.5.0
      volumeMounts:
        - name: "test-volume"
          mountPath: "/tmp"
    sparkConf:
      # Set this to "false" to retain pods for debugging
      "spark.kubernetes.executor.deleteOnTermination": "false"
      "spark.hadoop.fs.s3a.endpoint": "minio:9000"
      "spark.hadoop.fs.s3a.access.key": "minioadmin"
      "spark.hadoop.fs.s3a.secret.key": "minioadmin"
      "spark.hadoop.fs.s3a.connection.timeout": "600000"
      "spark.sql.debug.maxToStringFields": "100"
      "spark.hadoop.fs.s3a.path.style.access": "true"
      "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
      "spark.hadoop.fs.s3a.connection.ssl.enabled": "true"